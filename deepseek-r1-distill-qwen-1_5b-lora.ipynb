{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a617005e",
   "metadata": {},
   "source": [
    "## 导入依赖库\n",
    "\n",
    "这里导入相关的依赖库，主要为mindnlp， transformers，peft等常用库。transformers中定义了典型的transformer结构的模型，peft中实现了LoRA微调流程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cb221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindnlp\n",
    "import mindspore\n",
    "from mindnlp import core\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0245fc0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "The pyarrow installation is not built with support for the Parquet file format (DLL load failed while importing _parquet: 找不到指定的模块。)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\datasets\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.1.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Column, Dataset\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\datasets\\arrow_dataset.py:77\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcurrent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowReader\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowWriter, OptimizedTypedSequence\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_files\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_patterns\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\datasets\\arrow_reader.py:27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Optional, Union\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpq\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcurrent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DownloadConfig  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\pyarrow\\parquet\\__init__.py:20\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Licensed to the Apache Software Foundation (ASF) under one\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# or more contributor license agreements.  See the NOTICE file\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# distributed with this work for additional information\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\pyarrow\\parquet\\core.py:34\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_parquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_parquet\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe pyarrow installation is not built with support \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor the Parquet file format (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(exc)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     37\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_parquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ParquetReader, Statistics,  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     40\u001b[0m                               FileMetaData, RowGroupMetaData,\n\u001b[0;32m     41\u001b[0m                               ColumnChunkMetaData,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m                               FileDecryptionProperties,\n\u001b[0;32m     46\u001b[0m                               SortingColumn)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (LocalFileSystem, FileType, _resolve_filesystem_and_path,\n\u001b[0;32m     48\u001b[0m                         _ensure_filesystem)\n",
      "\u001b[1;31mImportError\u001b[0m: The pyarrow installation is not built with support for the Parquet file format (DLL load failed while importing _parquet: 找不到指定的模块。)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585597d7",
   "metadata": {},
   "source": [
    "## 下载并格式转换数据\n",
    "\n",
    "通过pd.read_json() 读取JSON文件到Pandas DataFrame，Dataset.from_pandas() 将DataFrame转换为Hugging Face的Dataset对象。\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64234c61",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 数据加载并进行格式转换\u001b[39;00m\n\u001b[0;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mextract-dialogue\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mconverted_sunwukong_lines_up.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m, lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m ds \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(df)\n\u001b[0;32m      4\u001b[0m ds[:\u001b[38;5;241m3\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# 数据加载并进行格式转换\n",
    "df = pd.read_json('d:\\\\extract-dialogue\\\\output\\\\converted_sunwukong_lines_up.jsonl', lines=True)\n",
    "ds = Dataset.from_pandas(df)\n",
    "ds[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2015af1c",
   "metadata": {},
   "source": [
    "## 实例化分词器\n",
    "\n",
    "实例化DeepSeek-R1-Distill-Qwen-1.5B的分词器，用于下一步的数据预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff97fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', use_fast=False, trust_remote_code=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c1dbe3",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "\n",
    "process_func函数的作用是将用户的指令（instruction）、输入内容（input）和模型的期望回复（output）格式化为适合语言模型训练的输入数据。主要包含以下处理：\n",
    "1. 对话模板构建：按system：[指令]，User: [输入] 和 Assistant: [回复] 的格式组织对话\n",
    "2. 分词与编码：用步骤3中的分词器将文本转换为模型可理解的数字序列\n",
    "3. 长度控制：截断超长内容\n",
    "4. 生成掩码和标签：标识有效内容和需要模型学习的部分\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a1e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 384    # Llama分词器会将一个中文字切分为多个token，因此需要放开一些最大长度，保证数据的完整性\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(f\"<|im_start|>system\\n现在你要扮演皇帝身边的女人--甄嬛<|im_end|>\\n<|im_start|>user\\n{example['instruction'] + example['input']}<|im_end|>\\n<|im_start|>assistant\\n\", add_special_tokens=False)  # add_special_tokens 不在开头加 special_tokens\n",
    "    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]  # 因为eos token咱们也是要关注的所以 补充为1\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]  \n",
    "    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa0d564",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_id = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d86d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenized_id[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af5616",
   "metadata": {},
   "source": [
    "## 模型加载\n",
    "\n",
    "此步骤是在配置Lora参数之前，需要先加载模型，本实验用DeepSeek-R1-Distill-Qwen-1.5B作为基础模型。加载时需要从镜像站下载模型的权重文件，耗时较长，请耐心等待。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载基础模型\n",
    "model = AutoModelForCausalLM.from_pretrained('deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', ms_dtype=mindspore.bfloat16, device_map=0)\n",
    "\n",
    "# 开启梯度检查点时，要执行该方法\n",
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3da2575",
   "metadata": {},
   "source": [
    "## 微调前推理\n",
    "\n",
    "进行模型推理查看效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ade8cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# host to device\n",
    "model = model.npu()\n",
    "\n",
    "prompt = \"你是谁？\"\n",
    "inputs = tokenizer.apply_chat_template([{\"role\": \"system\", \"content\": \"现在你要扮演皇帝身边的女人--甄嬛\"},{\"role\": \"user\", \"content\": prompt}],\n",
    "                                       add_generation_prompt=True,\n",
    "                                       tokenize=True,\n",
    "                                       return_tensors=\"ms\",\n",
    "                                       return_dict=True\n",
    "                                       ).to('cuda')\n",
    "\n",
    "\n",
    "gen_kwargs = {\"max_length\": 2500, \"do_sample\": True, \"top_k\": 1}\n",
    "with core.no_grad():\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e5603b",
   "metadata": {},
   "source": [
    "## 配置LoRA参数\n",
    "\n",
    "LoRA（Low-Rank Adaptation） 是一种高效微调大模型的技术，核心思想是冻结原始模型参数，通过向特定层注入低秩矩阵来实现参数更新，从而节省计算资源和内存。\n",
    "其中一些重要参数的意义如下：r用于控制低秩矩阵的维度、lora_alpha用于表示缩放因子、\n",
    "target_modules用于指定需要添加LoRA的层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8876f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置LoRA\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False, # 训练模式\n",
    "    r=8, # Lora 秩\n",
    "    lora_alpha=32, # Lora alaph，具体作用参见 Lora 原理\n",
    "    lora_dropout=0.1# Dropout 比例\n",
    ")\n",
    "config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e8df3a",
   "metadata": {},
   "source": [
    "对比前后模型变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d5294",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model without LoRA:\\n\",model)\n",
    "# 根据上述的lora配置，为模型添加lora部分\n",
    "model = get_peft_model(model, config)\n",
    "print('='*50)\n",
    "print(\"Model with LoRA:\\n\",model)\n",
    "# 输出打印需要训练的参数比例\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae05aa3f",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "\n",
    "本步骤中将完成训练中的参数配置，最终实例化Trainer启动模型的训练进程。\n",
    "首先，对训练中的参数进行配置，包括num_train_epochs训练轮次、learning_rate学习率、per_device_train_batch_size每批次的数据条数大小等。注意，训练中间过程输出的权重文件将会在output_dir参数指定的目录下。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd411cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练超参数\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./output_1.5bf/Qwen2.5_instruct_lora\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=5,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100, \n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667eea8e",
   "metadata": {},
   "source": [
    "最后，实例化Trainer类，将上述定义的参数传入，启动训练进程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce19594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_id,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa068d67",
   "metadata": {},
   "source": [
    "## 微调后推理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fa5869",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_path = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'\n",
    "lora_path = './output_1.5bf/Qwen2.5_instruct_lora/checkpoint-561' # 这里改称你的 lora 输出对应 checkpoint 地址\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(mode_path, trust_remote_code=True)\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(mode_path, ms_dtype=mindspore.bfloat16, trust_remote_code=True).eval()\n",
    "\n",
    "# 加载lora权重\n",
    "model = PeftModel.from_pretrained(model, model_id=lora_path)\n",
    "\n",
    "# host to device\n",
    "model = model.npu()\n",
    "\n",
    "prompt = \"你是谁？\"\n",
    "inputs = tokenizer.apply_chat_template([{\"role\": \"system\", \"content\": \"现在你要扮演皇帝身边的女人--甄嬛\"},{\"role\": \"user\", \"content\": prompt}],\n",
    "                                       add_generation_prompt=True,\n",
    "                                       tokenize=True,\n",
    "                                       return_tensors=\"ms\",\n",
    "                                       return_dict=True\n",
    "                                       ).to('cuda')\n",
    "\n",
    "\n",
    "gen_kwargs = {\"max_length\": 2500, \"do_sample\": True, \"top_k\": 1}\n",
    "with core.no_grad():\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
